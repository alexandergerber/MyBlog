---
title: Monte Carlo Simulations with purr and furrr
author: Alexander Gerber
date: '2019-08-23'
slug: monte-carlo-simulations-with-purr-and-furrr
categories: []
tags:
  - R
  - tidyverse
  - purrr
  - prallel
  - furrr
  - future
  - econometrics
cover: /img/parallel_stripes.jpg
draft: true
---

In the last post I showed how to use `purrr` to perform a simple Monte Carlo simulation. 
Since simulation studies are usually computationally quite expansive, it is usually benifical to
write efficient code and, maybe even more important when working on a modern computers, make use of parallelization.

I am working on a PC with a Ryzen 3700X processor with 8 cores and 16 threads and it would be a waste of
ressources not to go parallel. If we use `purrr` it is even extremly simple to do so as we will see. 

In the simulation study I will show why it is usually recommendable to use heteroscedasticity robust standard errors in linear regression. I don't want to go into the econometric details which can be found in every textbook.  

Similar to the last post I start by writing a function which 

1. Generates random data ( here according to the simple linear regression model with either homoskedastic or heteroscedastic errors. 
2. Does some statistical computations (here fitting a linear model with OLS, computeing standard errors, t-statistic and p-values)
3. Returns a data.frame with the results and the used parameters


```{r, eval = FALSE}
sample_t_stat <- function(n = 100, beta = c(0.5, 0.5), beta_0 = c(0, 0), error_dist = c("homo"), 
                          standard_error = c("normal")){
  
  # Generate data 
  X       <- cbind(rep(1,n), runif(n,-4,4))
  
  
  u <- switch(error_dist,
              homo     = rnorm(n, sd = 2),
              hetero   = rnorm(n, sd = sqrt(abs(X[ ,2]))), # same uncoditional variance as for homoscedasticity
              stop("Unknown distribution")
  )
  y       <- X %*% matrix(beta) + u
  
  # Fit the model with OLS
  lin_reg <- lm.fit(X,y)

  # Compute standard errors 
  se <- switch(standard_error,      
               normal = sqrt( (1/(n-2)*sum((lin_reg$residuals)^2) * solve(t(X) %*% X)[diag(T,2,2)])) ,
               robust = sqrt( (solve(t(X) %*% X) %*% t(X) %*% diag((lin_reg$residuals)^2) %*% X %*% solve(t(X) %*% X))[diag(T,2,2)]),
               stop("Unknown distribution")
  )
  
  # t-statistic and p-values
  t_stat  <- (lin_reg$coefficients - beta_0) / se
  p_value <- 2 * (1 - pnorm(abs(t_stat)))
  
  # beta_0 will bot be changed, therfore it doesen't have to be returned 
  data.frame(t_stat_beta1 = t_stat[1], t_stat_beta2 = t_stat[2],
             p_value_beta1 = p_value[1], p_value_beta2 = p_value[2], 
             n, error_dist, standard_error, beta1 = beta[1], beta2 = beta[2])
}
sample_t_stat()
```

Now, we have a function wich takes multiple parameters. We can look how the test performs with different sample sizes, 
error distribution, standard errors and coefficient values (here only $\beta_2$ is considered). 

```{r, eval = FALSE}
library(purrr)
sim_t_stat <- function(n, beta2, error_dist, standard_error){
  map_df(1:1000, ~ sample_t_stat(n = n,
                           beta = c(0, beta2), 
                           error_dist = error_dist, 
                           standard_error = standard_error)
      )
}
```




```{r, eval = FALSE}
parameter_grid <- expand.grid(
  n = c(10, 50, seq(100, 500, 100)),
  beta2 = seq(-0.5,0.5, 0.1), 
  error_dist = c("homo", "hetero"),
  standard_error = c("normal", "robust"), 
  stringsAsFactors = FALSE
  )
```


```{r, eval = FALSE}

pmap_df(parameter_grid, sim_t_stat)



```









